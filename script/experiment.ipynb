{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook eksperimen skripsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from fuzzywuzzy import fuzz\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/final_data/data_all.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung jumlah dan persentase match_type\n",
    "match_count = data['match_type'].value_counts()\n",
    "total_count = len(data)\n",
    "\n",
    "percentage_match = (match_count['Match'] / total_count) * 100 if 'Match' in match_count else 0\n",
    "percentage_non_match = (match_count['Non-Match'] / total_count) * 100 if 'Non-Match' in match_count else 0\n",
    "\n",
    "print(f\"Jumlah Baris Match: {match_count.get('Match', 0)} ({percentage_match:.2f}%)\")\n",
    "print(f\"Jumlah Baris Non-Match: {match_count.get('Non-Match', 0)} ({percentage_non_match:.2f}%)\")\n",
    "print(f\"Jumlah Baris: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename & Replace nama dan nilai kolom\n",
    "data.rename(columns={'match_type': 'match'}, inplace=True)\n",
    "data['match'] = data['match'].map({'Match': 1, 'Non-Match': 0})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models (Decission Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_levenshtein(row):\n",
    "    c1 = row['c1'].lower()\n",
    "    c2 = row['c2'].lower()\n",
    "    return Levenshtein.distance(c1, c2)\n",
    "\n",
    "data['levensthein_distance'] = data.apply(calculate_levenshtein, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Split and Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih fitur dan target\n",
    "features = data[['levensthein_distance']] \n",
    "target = data['match']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Melatih model Decision Tree\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Melakukan prediksi\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Similarity (with Jaccard)\n",
    "Buat n-gram, lalu cari distancenya menggunakan Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membuat n-gram (Bi-Gram 2)\n",
    "def ngram_generator(text, n=2):\n",
    "    \"\"\"\n",
    "    text: String yang akan dipecah menjadi n-gram\n",
    "    n : panjang n-gram yang akan di buat\n",
    "    \"\"\"\n",
    "    return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "# Fungsi untuk menghitung n-gram similarity\n",
    "def calculate_ngram_similarity(row):\n",
    "    \"\"\"\n",
    "    row: baris pada dataframe (c1,c2)\n",
    "    n: panjang n-gram\n",
    "    \"\"\"\n",
    "\n",
    "    c1 = row['c1'].lower()\n",
    "    c2 = row['c2'].lower()\n",
    "\n",
    "    ngrams_c1 = ngram_generator(c1)\n",
    "    ngrams_c2 = ngram_generator(c2)\n",
    "\n",
    "    # Menggunakan MultiLabelBinarizer untuk menghitung Jaccard similarity\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    # ngrams = list(set(ngrams_c1) | set(ngrams_c2))  # Semua n-gram unik\n",
    "    binarized = mlb.fit_transform([ngrams_c1, ngrams_c2])\n",
    "    \n",
    "    return jaccard_score(binarized[0], binarized[1])\n",
    "\n",
    "data['ngram_similarity'] = data.apply(calculate_ngram_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Split and Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih fitur dan target\n",
    "features = data[['ngram_similarity']] \n",
    "target = data['match']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Melatih model Decision Tree\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Melakukan prediksi\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Sort Ratio\n",
    "\n",
    "Token sort ratio tidak peduli urutan kata dalam suatu kalimat, Cocok untuk company matching\n",
    "\n",
    "Contoh:\n",
    "Bank Mandiri - Mandiri Bank\n",
    "\n",
    "Bank Central Asia - Asia Central Bank\n",
    "\n",
    "Bayer AG - AG Bayer\n",
    "\n",
    "Nestlé S.A. - S.A. Nestlé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the similarity score with different names\n",
    "full_name_1 = \"Alice B. Johnson\"\n",
    "full_name_2 = \"Johnson Alice B.\"\n",
    "\n",
    "# Order does not matter for token sort ratio\n",
    "print(f\"Token sort ratio similarity score: {fuzz.token_sort_ratio(full_name_2, full_name_1)}\")\n",
    "\n",
    "# Order matters for partial ratio\n",
    "print(f\"Partial ratio similarity score: {fuzz.partial_ratio(full_name_1, full_name_2)}\")\n",
    "\n",
    "# Order will not affect simple ratio if strings do not match\n",
    "print(f\"Simple ratio similarity score: {fuzz.ratio(full_name_1, full_name_2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung token sort ratio\n",
    "def calculate_token_partial_ratio(row):\n",
    "    c1 = row['c1'].lower()\n",
    "    c2 = row['c2'].lower()\n",
    "    return fuzz.partial_ratio(c1, c2)\n",
    "\n",
    "data['token_partial_ratio'] = data.apply(calculate_token_partial_ratio, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Split and Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih fitur dan target\n",
    "features = data[['token_partial_ratio']] \n",
    "target = data['match']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Melatih model Decision Tree\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Melakukan prediksi\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = data[[\"c1\",\"c2\",\"levensthein_distance\",\"ngram_similarity\",\"token_partial_ratio\",\"match\"]]\n",
    "selected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menambahkan fitur baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung jumlah kata dan jumlah karakter\n",
    "data['number_of_words_c1'] = data['c1'].apply(lambda x: len(x.split()))\n",
    "data['number_of_characters_c1'] = data['c1'].apply(lambda x: len(x))\n",
    "\n",
    "data['number_of_words_c2'] = data['c2'].apply(lambda x: len(x.split()))\n",
    "data['number_of_characters_c2'] = data['c2'].apply(lambda x: len(x))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "Combine all feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih fitur dan target\n",
    "features = data[['levensthein_distance','ngram_similarity','token_partial_ratio']] \n",
    "target = data['match']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Melatih model Decision Tree\n",
    "model = XGBClassifier(eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Melakukan prediksi\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih fitur dan target\n",
    "features = data[['levensthein_distance','ngram_similarity','token_partial_ratio', 'number_of_words_c1', 'number_of_words_c2', 'number_of_characters_c1', 'number_of_characters_c2']] \n",
    "target = data['match']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Melatih model Decision Tree\n",
    "model = XGBClassifier(eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Melakukan prediksi\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "1. Pre-processing context (tanda baca, dll)\n",
    "2. Tokenization (perlukah ?)\n",
    "3. Vectorization\n",
    "4. Training BERT model\n",
    "5. Testing/Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanpa Konteks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buat Fitur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Embeddings (c1,c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# import torch\n",
    "\n",
    "# # Memuat tokenizer dan model BERT\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Fungsi untuk mendapatkan embedding dari teks menggunakan BERT\n",
    "# def get_embedding(text):\n",
    "#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     # Mengambil embedding dari token [CLS] (index 0)\n",
    "#     return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# # Menghitung embedding untuk kolom c1 dan c2\n",
    "# data['c1_embedding'] = data['c1'].apply(lambda x: get_embedding(x))\n",
    "# data['c2_embedding'] = data['c2'].apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Embeddings (c1_context, c2_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Memuat tokenizer dan model BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Fungsi untuk mendapatkan embedding dari teks menggunakan BERT\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mengambil embedding dari token [CLS] (index 0)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Menghitung embedding untuk kolom c1 dan c2\n",
    "data['c1_context_embedding'] = data['c1_context'].apply(lambda x: get_embedding(x))\n",
    "data['c2_context_embedding'] = data['c2_context'].apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah embedding menjadi array NumPy\n",
    "data['c1_context_embedding'] = data['c1_context_embedding'].apply(lambda x: np.array(x))\n",
    "data['c2_context_embedding'] = data['c2_context_embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "# Mengubah embedding menjadi array NumPy\n",
    "data['c1_embedding'] = data['c1_embedding'].apply(lambda x: np.array(x))\n",
    "data['c2_embedding'] = data['c2_embedding'].apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('../data/final_data/temp.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/final_data/temp.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung cosine similarity\n",
    "def cosine_similarity(c1_embedding, c2_embedding):\n",
    "    c1_embedding = np.array(c1_embedding).flatten()\n",
    "    c2_embedding = np.array(c2_embedding).flatten()\n",
    "    return 1 - cosine(c1_embedding, c2_embedding)  # 1 - cosine distance\n",
    "\n",
    "data['cosine_similarity'] = data.apply(lambda row: cosine_similarity(row['c1_embedding'], row['c2_embedding']), axis=1)\n",
    "\n",
    "# Buat DataFrame baru\n",
    "deep_learning_no_context = data[['c1', 'c2', 'c1_embedding', 'c2_embedding', 'cosine_similarity', 'match']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learning_no_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learning_no_context.to_json('../data/final_data/deep_learning_no_context.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain cosine similarity bisa tambahkan:\n",
    "- L1 (Manhattan Distance)\n",
    "- L2 (Euclidean Distance)\n",
    "- L-infinity (Chebyshev Distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dengan Konteks\n",
    "Coba dengan dan tanpa preprocessing context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "Real Use Case: ketika ada 25000an entitas, dan dikasih 1 entitas, diminta untuk matching dia sebenernya entitas yang mana.\n",
    "\n",
    "Untuk case fauzan: Setelah coba selesaikan binary classif, masuk ke real use case.\n",
    "\n",
    "-Kendala:\n",
    "Kalau pakai algo machine learning (XGBoost), gimana cara lookup data.\n",
    "\n",
    "Ketika punya data baru, bandingin dengan cosine similarity (mana yg dekat)\n",
    "\n",
    "\n",
    "Evaluations:\n",
    "- Binary classification hitnung accuracy, dll\n",
    "- Evaluasi look up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "Model bert yang udah di pretrained, \n",
    "\n",
    "2500 entity (embedded) di store di numpy array untuk lookup. (metrics pake precission @1, @10 & recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "company-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
