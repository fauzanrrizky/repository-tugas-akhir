{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Fauzan\\anaconda3\\envs\\company-matching\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from fuzzywuzzy import fuzz\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import BertForSequenceClassification, AdamW, BertTokenizer, DistilBertForSequenceClassification, DistilBertTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/final_data/temp.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anz</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anz</td>\n",
       "      <td>Australian New Zealand Banking Group</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anz</td>\n",
       "      <td>A.N.Z.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anz</td>\n",
       "      <td>ANZ Bank</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anz</td>\n",
       "      <td>Anz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23229</th>\n",
       "      <td>aerotek</td>\n",
       "      <td>AeroVironment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23230</th>\n",
       "      <td>aerotek</td>\n",
       "      <td>Aeropostale</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23231</th>\n",
       "      <td>michigan state university</td>\n",
       "      <td>Michigan Technological University</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23232</th>\n",
       "      <td>michigan state university</td>\n",
       "      <td>Central Michigan University</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23233</th>\n",
       "      <td>michigan state university</td>\n",
       "      <td>Michigan State Police</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23234 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              c1                                    c2  match\n",
       "0                            anz                                   ANZ      1\n",
       "1                            anz  Australian New Zealand Banking Group      1\n",
       "2                            anz                                A.N.Z.      1\n",
       "3                            anz                              ANZ Bank      1\n",
       "4                            anz                                   Anz      1\n",
       "...                          ...                                   ...    ...\n",
       "23229                    aerotek                         AeroVironment      0\n",
       "23230                    aerotek                           Aeropostale      0\n",
       "23231  michigan state university     Michigan Technological University      0\n",
       "23232  michigan state university           Central Michigan University      0\n",
       "23233  michigan state university                 Michigan State Police      0\n",
       "\n",
       "[23234 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_without_context = data[['c1','c2','match']]\n",
    "bert_without_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mfauz\\AppData\\Local\\Temp\\ipykernel_1996\\1177542444.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bert_without_context['combined'] = bert_without_context['c1'] + \" [SEP] \" + bert_without_context['c2']\n",
      "d:\\Fauzan\\anaconda3\\envs\\company-matching\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18587/18587 [00:07<00:00, 2478.41 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4647/4647 [00:01<00:00, 3424.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Combine c1 and c2 into a single string with a separator\n",
    "bert_without_context['combined'] = bert_without_context['c1'] + \" [SEP] \" + bert_without_context['c2']\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_df, test_df = train_test_split(bert_without_context, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare for HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['combined', 'match']].reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df[['combined', 'match']].reset_index(drop=True))\n",
    "\n",
    "# Load BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenisasi teks\n",
    "    tokens = tokenizer(examples['combined'], padding='max_length', truncation=True)\n",
    "    # Tambahkan label ke token output\n",
    "    tokens['labels'] = examples['match']\n",
    "    return tokens\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT model with classification head\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,  # Optional, jika ingin menggunakan tokenizer secara eksplisit\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model dan tokenizer dari direktori yang sudah disimpan\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"../model/distillbert_no_context\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"../model/distillbert_no_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Fauzan\\anaconda3\\envs\\company-matching\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 581/581 [01:19<00:00,  7.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Set arguments untuk evaluasi saja\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                # Directory untuk menyimpan hasil evaluasi (misalnya metrics)\n",
    "    per_device_eval_batch_size=8,          # Ukuran batch untuk evaluasi\n",
    "    do_train=False,                        # Jangan lakukan training\n",
    "    do_eval=True,                          # Hanya lakukan evaluasi\n",
    "    evaluation_strategy=\"no\",              # Evaluasi tidak per epoch karena Anda tidak sedang melatih ulang\n",
    "    logging_dir='./logs',                  # Directory untuk menyimpan logs\n",
    "    report_to=\"none\"                       # Tidak perlu logging ke tempat lain (misalnya WandB)\n",
    ")\n",
    "\n",
    "# Define the function to compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37126779556274414, 'eval_model_preparation_time': 0.002, 'eval_accuracy': 0.9162900796212611, 'eval_runtime': 80.3665, 'eval_samples_per_second': 57.823, 'eval_steps_per_second': 7.229}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siapkan input teks untuk prediksi\n",
    "c1 = \"bank niaga\"\n",
    "c2 = \"cimb niaga\"\n",
    "\n",
    "# Gabungkan c1 dan c2 dengan separator seperti yang digunakan selama pelatihan\n",
    "input_text = c1 + \" [SEP] \" + c2\n",
    "\n",
    "# Tokenisasi teks\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Lakukan inference (pastikan model dalam mode evaluasi)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Ambil logits dari output\n",
    "logits = outputs.logits\n",
    "\n",
    "# Konversi logits ke prediksi kelas (0 atau 1)\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Lihat hasil prediksi\n",
    "print(f\"Prediksi: {predictions.item()}\")  # Prediksi bisa 0 atau 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../model/distillbert_no_context\")\n",
    "tokenizer.save_pretrained(\"../model/distillbert_no_context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c1_context</th>\n",
       "      <th>c2_context</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anz</td>\n",
       "      <td>ANZ</td>\n",
       "      <td>The Australia and New Zealand Banking Group Li...</td>\n",
       "      <td>We provide banking and financial products and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anz</td>\n",
       "      <td>Australian New Zealand Banking Group</td>\n",
       "      <td>The Australia and New Zealand Banking Group Li...</td>\n",
       "      <td>ANZ\\r\\n- The Australian New Zealand Banking Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anz</td>\n",
       "      <td>A.N.Z.</td>\n",
       "      <td>The Australia and New Zealand Banking Group Li...</td>\n",
       "      <td>We provide banking and financial products and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anz</td>\n",
       "      <td>ANZ Bank</td>\n",
       "      <td>The Australia and New Zealand Banking Group Li...</td>\n",
       "      <td>ANZ Bank New Zealand Limited operates as a ban...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anz</td>\n",
       "      <td>Anz</td>\n",
       "      <td>The Australia and New Zealand Banking Group Li...</td>\n",
       "      <td>We provide banking and financial products and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23229</th>\n",
       "      <td>aerotek</td>\n",
       "      <td>AeroVironment</td>\n",
       "      <td>Aerotek\\nprovides staffing and services soluti...</td>\n",
       "      <td>AeroVironment (NASDAQ: AVAV) is a technology s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23230</th>\n",
       "      <td>aerotek</td>\n",
       "      <td>Aeropostale</td>\n",
       "      <td>Aerotek\\nprovides staffing and services soluti...</td>\n",
       "      <td>AÃ©ropostale is a specialty retailer of high-qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23231</th>\n",
       "      <td>michigan state university</td>\n",
       "      <td>Michigan Technological University</td>\n",
       "      <td>Michigan State University is the nation's prem...</td>\n",
       "      <td>Michigan Technological University is a flagshi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23232</th>\n",
       "      <td>michigan state university</td>\n",
       "      <td>Central Michigan University</td>\n",
       "      <td>Michigan State University is the nation's prem...</td>\n",
       "      <td>Central Michigan University is a leading publi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23233</th>\n",
       "      <td>michigan state university</td>\n",
       "      <td>Michigan State Police</td>\n",
       "      <td>Michigan State University is the nation's prem...</td>\n",
       "      <td>The Michigan State Police (MSP) is a full-serv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23234 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              c1                                    c2  \\\n",
       "0                            anz                                   ANZ   \n",
       "1                            anz  Australian New Zealand Banking Group   \n",
       "2                            anz                                A.N.Z.   \n",
       "3                            anz                              ANZ Bank   \n",
       "4                            anz                                   Anz   \n",
       "...                          ...                                   ...   \n",
       "23229                    aerotek                         AeroVironment   \n",
       "23230                    aerotek                           Aeropostale   \n",
       "23231  michigan state university     Michigan Technological University   \n",
       "23232  michigan state university           Central Michigan University   \n",
       "23233  michigan state university                 Michigan State Police   \n",
       "\n",
       "                                              c1_context  \\\n",
       "0      The Australia and New Zealand Banking Group Li...   \n",
       "1      The Australia and New Zealand Banking Group Li...   \n",
       "2      The Australia and New Zealand Banking Group Li...   \n",
       "3      The Australia and New Zealand Banking Group Li...   \n",
       "4      The Australia and New Zealand Banking Group Li...   \n",
       "...                                                  ...   \n",
       "23229  Aerotek\\nprovides staffing and services soluti...   \n",
       "23230  Aerotek\\nprovides staffing and services soluti...   \n",
       "23231  Michigan State University is the nation's prem...   \n",
       "23232  Michigan State University is the nation's prem...   \n",
       "23233  Michigan State University is the nation's prem...   \n",
       "\n",
       "                                              c2_context  match  \n",
       "0      We provide banking and financial products and ...      1  \n",
       "1      ANZ\\r\\n- The Australian New Zealand Banking Gr...      1  \n",
       "2      We provide banking and financial products and ...      1  \n",
       "3      ANZ Bank New Zealand Limited operates as a ban...      1  \n",
       "4      We provide banking and financial products and ...      1  \n",
       "...                                                  ...    ...  \n",
       "23229  AeroVironment (NASDAQ: AVAV) is a technology s...      0  \n",
       "23230  AÃ©ropostale is a specialty retailer of high-qu...      0  \n",
       "23231  Michigan Technological University is a flagshi...      0  \n",
       "23232  Central Michigan University is a leading publi...      0  \n",
       "23233  The Michigan State Police (MSP) is a full-serv...      0  \n",
       "\n",
       "[23234 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_with_context = data[['c1','c2','c1_context','c2_context','match']]\n",
    "bert_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mfauz\\AppData\\Local\\Temp\\ipykernel_3412\\1327821698.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bert_with_context['combined'] = bert_with_context['c1'] + \": \" + bert_with_context['c1_context']\\\n",
      "d:\\Fauzan\\anaconda3\\envs\\company-matching\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18587/18587 [00:33<00:00, 562.23 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4647/4647 [00:08<00:00, 569.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Combine c1: context and c2: context into a single string with a separator\n",
    "bert_with_context['combined'] = bert_with_context['c1'] + \": \" + bert_with_context['c1_context']\\\n",
    "+ \" [SEP] \" + bert_with_context['c2'] + \": \" + bert_with_context['c2_context']\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_df, test_df = train_test_split(bert_with_context, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare for HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['combined', 'match']].reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df[['combined', 'match']].reset_index(drop=True))\n",
    "\n",
    "# Load BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenisasi teks\n",
    "    tokens = tokenizer(examples['combined'], padding='max_length', truncation=True)\n",
    "    # Tambahkan label ke token output\n",
    "    tokens['labels'] = examples['match']\n",
    "    return tokens\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Fauzan\\anaconda3\\envs\\company-matching\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  7%|â–‹         | 500/6972 [53:55<11:23:19,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4462, 'grad_norm': 11.03742790222168, 'learning_rate': 1.8565691336775675e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 1000/6972 [2:21:50<17:57:39, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2636, 'grad_norm': 0.7353519797325134, 'learning_rate': 1.7131382673551347e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 1500/6972 [3:51:58<16:35:57, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2403, 'grad_norm': 39.95343017578125, 'learning_rate': 1.5697074010327024e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–Š       | 2000/6972 [5:16:02<13:58:57, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2045, 'grad_norm': 0.05903494730591774, 'learning_rate': 1.4262765347102698e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2324/6972 [6:33:06<10:12:25,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16455991566181183, 'eval_runtime': 1312.7613, 'eval_samples_per_second': 3.54, 'eval_steps_per_second': 0.443, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 2500/6972 [7:02:37<12:31:26, 10.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1828, 'grad_norm': 0.040902599692344666, 'learning_rate': 1.2828456683878371e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3000/6972 [8:30:37<11:32:54, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1324, 'grad_norm': 0.06743989139795303, 'learning_rate': 1.1394148020654046e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3500/6972 [10:01:15<10:50:14, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1274, 'grad_norm': 0.032925091683864594, 'learning_rate': 9.959839357429719e-06, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4000/6972 [11:28:24<8:33:24, 10.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.108, 'grad_norm': 6.1386027336120605, 'learning_rate': 8.525530694205393e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 4500/6972 [12:45:41<5:21:09,  7.80s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1184, 'grad_norm': 0.04444197192788124, 'learning_rate': 7.091222030981068e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4648/6972 [13:53:19<5:01:40,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16713140904903412, 'eval_runtime': 1279.8028, 'eval_samples_per_second': 3.631, 'eval_steps_per_second': 0.454, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5000/6972 [14:52:53<5:32:15, 10.11s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.053, 'grad_norm': 0.025715775787830353, 'learning_rate': 5.656913367756741e-06, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 5500/6972 [16:17:18<4:06:42, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0725, 'grad_norm': 0.026789914816617966, 'learning_rate': 4.222604704532416e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6000/6972 [17:42:08<2:45:48, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.067, 'grad_norm': 19.058313369750977, 'learning_rate': 2.7882960413080896e-06, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 6500/6972 [19:06:16<1:18:47, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0668, 'grad_norm': 0.023077568039298058, 'learning_rate': 1.3539873780837638e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6972/6972 [20:47:31<00:00,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16030742228031158, 'eval_runtime': 1310.8114, 'eval_samples_per_second': 3.545, 'eval_steps_per_second': 0.443, 'epoch': 3.0}\n",
      "{'train_runtime': 74851.1349, 'train_samples_per_second': 0.745, 'train_steps_per_second': 0.093, 'train_loss': 0.15340027040403872, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6972/6972 [20:47:31<00:00, 10.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6972, training_loss=0.15340027040403872, metrics={'train_runtime': 74851.1349, 'train_samples_per_second': 0.745, 'train_steps_per_second': 0.093, 'total_flos': 7386514616457216.0, 'train_loss': 0.15340027040403872, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT model with classification head\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer, \n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/distillbert_with_context\\\\tokenizer_config.json',\n",
       " '../model/distillbert_with_context\\\\special_tokens_map.json',\n",
       " '../model/distillbert_with_context\\\\vocab.txt',\n",
       " '../model/distillbert_with_context\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../model/distillbert_with_context\")\n",
    "tokenizer.save_pretrained(\"../model/distillbert_with_context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Fauzan\\anaconda3\\envs\\company-matching\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 581/581 [21:46<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Set arguments untuk evaluasi saja\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                # Directory untuk menyimpan hasil evaluasi (misalnya metrics)\n",
    "    per_device_eval_batch_size=8,          # Ukuran batch untuk evaluasi\n",
    "    do_train=False,                        # Jangan lakukan training\n",
    "    do_eval=True,                          # Hanya lakukan evaluasi\n",
    "    evaluation_strategy=\"no\",              # Evaluasi tidak per epoch karena Anda tidak sedang melatih ulang\n",
    "    logging_dir='./logs',                  # Directory untuk menyimpan logs\n",
    "    report_to=\"none\"                       # Tidak perlu logging ke tempat lain (misalnya WandB)\n",
    ")\n",
    "\n",
    "# Define the function to compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16030742228031158, 'eval_model_preparation_time': 0.0029, 'eval_accuracy': 0.9685818807833011, 'eval_runtime': 1308.7915, 'eval_samples_per_second': 3.551, 'eval_steps_per_second': 0.444}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "company-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
