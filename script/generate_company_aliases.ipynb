{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw_data/companies_sorted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Companies from latin only name countries\n",
    "\n",
    "Untuk setiap negara, Saya mengambil 200 perusahaan dengan jumlah employee terbesar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**G20 Latin Only Names:**\n",
    "1. united states\n",
    "2. germany\n",
    "3. india\n",
    "4. united kingdom\n",
    "5. france\n",
    "6. russia\n",
    "7. canada\n",
    "8. italy\n",
    "9. brazil\n",
    "10. australia\n",
    "11. mexico\n",
    "12. spain\n",
    "13. indonesia\n",
    "14. netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List eligible countries\n",
    "countries = ['united states', 'germany', 'india', 'united kingdom', 'france', 'russia', 'canada', 'italy', 'brazil', 'australia', 'mexico', 'spain', 'indonesia', 'netherlands']\n",
    "\n",
    "# Filter rows dengan country sesuai dalam list\n",
    "eligible_countries = df[df['country'].str.lower().isin(countries)]\n",
    "\n",
    "# Sort and get top 200 companies for each country\n",
    "company_names_g20 = (eligible_countries.groupby('country', group_keys=False)\n",
    "                        .apply(lambda x: x.sort_values('total employee estimate', ascending=False).head(200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil nilai name dan jadikan ke list\n",
    "company_canonical_list = company_names_g20['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_canonical_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/raw_data/company_canonical.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for item in company_canonical_list:\n",
    "        writer.writerow([item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_prompt(company_canonical_list):\n",
    "\n",
    "    return f\"\"\"\"\n",
    "    You are an expert in company names and their aliases. Your task is to help me create a precise dataset of company names and their aliases. For each company name, generate at least seven variations that refers to SAME entities with typographical error, mispellings, abbreviations, acronyms, and official legal names. The goal is to capture all possible variations that would be considered a match to the original company name.\n",
    "    Your output should be in a CSV table format with three fields: company_name, alias_company_name, and match_type. Label each pair as \"Match\" in the match_type field. If there are multiple non-matching aliases for the same company, add each alias on a separate row while maintaining the three columns.\n",
    "    The output style should be simple, like this: company_name,alias_company_name,match_type\n",
    "    Tesla,Telsa,Match\n",
    "    Fujitsu,Fujutsu,Match\n",
    "    Telstra,Telesta,Match\n",
    "    Intesa Sanpaolo,Intesa San Paolo,Match\n",
    "    Fiat Chrysler Automobiles,FCA,Match\n",
    "    OpenAI,Open AI,Match\n",
    "    Google,Google LLC,Match\n",
    "    Apple Inc.,Apple,Match\n",
    "    Microsoft Corporation,MSFT,Match\n",
    "    Amazon.com,Amazon,Match\n",
    "    Commonwealth bank,CommBank,Match\n",
    "\n",
    "    ----------------------------------------------------------\n",
    "\n",
    "    List of company names: {company_canonical_list}\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def run_prompt_for_chunk(chunk):\n",
    "    pair_prompt = get_pair_prompt(chunk)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": pair_prompt}\n",
    "        ],\n",
    "        max_tokens=2300,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=1,\n",
    "        stream=False\n",
    "    )\n",
    "    # Mengambil konten dari response\n",
    "    content = response.choices[0].message.content\n",
    "    print(content)\n",
    "    return content\n",
    "\n",
    "def save_and_concat(content, output_file):\n",
    "    lines = content.strip().split(\"\\n\")\n",
    "    \n",
    "    # Periksa apakah setiap baris memiliki tepat 3 elemen yang dipisahkan koma\n",
    "    valid_lines = []\n",
    "    for line in lines:\n",
    "        parts = line.split(',')\n",
    "        if len(parts) == 3:\n",
    "            valid_lines.append(parts)\n",
    "        else:\n",
    "            print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Buat DataFrame dari baris valid\n",
    "    df = pd.DataFrame(valid_lines, columns=[\"company_name\", \"alias_company_name\", \"match_type\"])\n",
    "    \n",
    "    # Append to the CSV file\n",
    "    with open(output_file, 'a', newline='') as f:\n",
    "        df.to_csv(f, header=f.tell()==0, index=False)\n",
    "\n",
    "# Iterasi setiap 20 elemen dalam daftar\n",
    "chunks = [company_canonical_list[i:i + 20] for i in range(0, len(company_canonical_list), 20)]\n",
    "output_file = \"../generated_data/company_match.csv\"\n",
    "\n",
    "iteration = 0\n",
    "for chunk in chunks:\n",
    "    iteration += 1\n",
    "    content = run_prompt_for_chunk(chunk)\n",
    "    save_and_concat(content, output_file)\n",
    "    print(\"Done iteration \" + str(iteration))\n",
    "\n",
    "print(f\"All data has been concatenated into {output_file}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonpair_prompt(company_canonical_list):\n",
    "\n",
    "    return f\"\"\"\"\n",
    "    You are an expert in company names and their aliases. Your task is to help me create a dataset of company names and their possible wrong matches. For each company name, generate at least five alias company that LITTLE SIMILAR but refers to different entity (Real entity). The goal is to capture all possible variations that would NOT be considered a match to the original company name. \n",
    "    Your output should be in a CSV table format with three fields: company_name, alias_company_name, and match_type. Label each pair as \"Non-Match\" in the match_type field. If there are multiple non-matching aliases for the same company, add each alias on a separate row while maintaining the three columns. \n",
    "    The output style should be simple, like this: company_name,alias_company_name,match_type \n",
    "    fujitsu, Fujifilm, Non-Match\n",
    "    national australia bank, National Bank of Canada, Non-Match\n",
    "    OpenAI,Google AI,Non-Match \n",
    "    Google,Apple,Non-Match \n",
    "    Bank Rakyat Indonesia,Bank Negara Indonesia,Non-Match\n",
    "    Microsoft Corporation,Microchip Technologies,Non-Match \n",
    "    New York Mets, New York Yankees,Non-Match \n",
    "    Alianz, Asus \n",
    "    ----------------------------------------------------------\n",
    "\n",
    "    List of company names: {company_canonical_list}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def run_prompt_for_chunk(chunk):\n",
    "    non_pair_prompt = get_nonpair_prompt(chunk)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": non_pair_prompt}\n",
    "        ],\n",
    "        max_tokens=2300,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=1,\n",
    "        stream=False\n",
    "    )\n",
    "    # Mengambil konten dari response\n",
    "    content = response.choices[0].message.content\n",
    "    print(content)\n",
    "    return content\n",
    "\n",
    "def save_and_concat(content, output_file):\n",
    "    lines = content.strip().split(\"\\n\")\n",
    "    \n",
    "    # Periksa apakah setiap baris memiliki tepat 3 elemen yang dipisahkan koma\n",
    "    valid_lines = []\n",
    "    for line in lines:\n",
    "        parts = line.split(',')\n",
    "        if len(parts) == 3:\n",
    "            valid_lines.append(parts)\n",
    "        else:\n",
    "            print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Buat DataFrame dari baris valid\n",
    "    df = pd.DataFrame(valid_lines, columns=[\"company_name\", \"alias_company_name\", \"match_type\"])\n",
    "    \n",
    "    # Append to the CSV file\n",
    "    with open(output_file, 'a', newline='') as f:\n",
    "        df.to_csv(f, header=f.tell()==0, index=False)\n",
    "\n",
    "# Iterasi setiap 20 elemen dalam daftar\n",
    "chunks = [company_canonical_list[i:i + 20] for i in range(0, len(company_canonical_list), 20)]\n",
    "output_file = \"../generated_data/company_non_match.csv\"\n",
    "\n",
    "iteration = 0\n",
    "for chunk in chunks:\n",
    "    iteration += 1\n",
    "    content = run_prompt_for_chunk(chunk)\n",
    "    save_and_concat(content, output_file)\n",
    "    print(\"Done iteration \" + str(iteration))\n",
    "\n",
    "print(f\"All data has been concatenated into {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = pd.read_csv('../data/generated_data/company_match.csv')\n",
    "\n",
    "# Drop row yang kolomnya > 3\n",
    "non_match_df = pd.read_csv('../data/generated_data/company_non_match.csv', on_bad_lines='skip')\n",
    "non_match_df = non_match_df[non_match_df.apply(lambda x: len(x.dropna()) == 3, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop baris yang nilai kolom 'company_name' adalah \"company_name\"\n",
    "match_df = match_df[match_df['company_name'] != 'company_name']\n",
    "non_match_df = non_match_df[non_match_df['company_name'] != 'company_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan dataframe\n",
    "company_alias = pd.concat([match_df, non_match_df], ignore_index=True)\n",
    "\n",
    "# Simpan Dataframe\n",
    "company_alias.to_csv('../data/generated_data/company_alias.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
